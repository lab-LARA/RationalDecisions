{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4j1mKx_Slkg",
        "outputId": "f97724c0-e3f8-40e9-a305-372a41e400eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.8.1-py3-none-any.whl (970 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m970.4/970.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji (from stanza)\n",
            "  Downloading emoji-2.11.0-py2.py3-none-any.whl (433 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.31.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza) (3.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from stanza) (0.10.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.66.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3.0->stanza)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, emoji, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stanza\n",
            "Successfully installed emoji-2.11.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 stanza-1.8.1\n"
          ]
        }
      ],
      "source": [
        "pip install stanza"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AXPvJ8FW9iT",
        "outputId": "90935602-80d6-406f-fbbb-865bc6a28ad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUV3xKXTXD9L",
        "outputId": "b5b6154e-6bba-4a5f-ea85-10a77a572b04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.4.2\n"
          ]
        }
      ],
      "source": [
        "pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rjh527NVXXgE"
      },
      "outputs": [],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9yuz5fSby_R",
        "outputId": "071efd7a-a075-42d4-8c49-dee85ea131d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb00r33ob1QE",
        "outputId": "ead62d5c-1ffa-4e68-b1cc-194dbdd1f4fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIhBfP_XUIuG",
        "outputId": "e379baa6-50b1-4434-fdce-fb07a7492009"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import stanza\n",
        "from datasets import *\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6HCWWqC0AKe"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/NYT_Dataset.csv\",index_col = 0, parse_dates = [\"Date\"],na_values=['nan'])\n",
        "train, test = train_test_split(df, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354,
          "referenced_widgets": [
            "a4968cd6a6084aeabacb381dbc42ddda",
            "c407ad7b04734809a0f9fa08d209a3a2",
            "9c7f55e34b4949cfaa726a25f93b5568",
            "1d61fc96cdb24f89a8a0b0fde5d2fe77",
            "6e7d5ddd4eca45d48b269922fe3fa407",
            "0ec91a71eec74c9fbb7cb6214d30efdb",
            "0c15fb8e26c44b4cb34e1bf942a0debc",
            "5bba7a1b6c694a26b8659441021c6744",
            "bbf0e519b5f047199f52c3a439a50057",
            "e8181f280c06475584bec182b241b191",
            "d20234d1ab4849f688905d5b4b0340c7"
          ]
        },
        "id": "wU_8-4V1VXLv",
        "outputId": "580a840a-2170-44fc-db5c-1a88e2b0526f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4968cd6a6084aeabacb381dbc42ddda",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "WARNING:stanza:Language en package default expects mwt, which has been added\n",
            "INFO:stanza:Loading these models for language: en (English):\n",
            "=========================================\n",
            "| Processor | Package                   |\n",
            "-----------------------------------------\n",
            "| tokenize  | combined                  |\n",
            "| mwt       | combined                  |\n",
            "| ner       | ontonotes-ww-multi_charlm |\n",
            "=========================================\n",
            "\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Loading: ner\n",
            "INFO:stanza:Done loading processors!\n"
          ]
        }
      ],
      "source": [
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNlmUNGmYS9f"
      },
      "outputs": [],
      "source": [
        "d = {}\n",
        "for x in train['title']:\n",
        "  doc = nlp(x)\n",
        "  print(*[f'entity: {ent.text}\\ttype: {ent.type}' for ent in doc.ents], sep='\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMHFUpHYbx7b"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "spacy.load('en_core_web_sm')\n",
        "from spacy.lang.en import English\n",
        "parser = English()\n",
        "def tokenize(text):\n",
        "    lda_tokens = []\n",
        "    tokens = parser(text)\n",
        "    for token in tokens:\n",
        "        if token.orth_.isspace():\n",
        "            continue\n",
        "        elif token.like_url:\n",
        "            lda_tokens.append('URL')\n",
        "        elif token.orth_.startswith('@'):\n",
        "            lda_tokens.append('SCREEN_NAME')\n",
        "        else:\n",
        "            lda_tokens.append(token.lower_)\n",
        "    return lda_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnUnliURcP6_",
        "outputId": "a5832330-6183-4604-b839-26b933b3097e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "def get_lemma(word):\n",
        "    lemma = wn.morphy(word)\n",
        "    if lemma is None:\n",
        "        return word\n",
        "    else:\n",
        "        return lemma\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "def get_lemma2(word):\n",
        "    return WordNetLemmatizer().lemmatize(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m6a30zecUZW",
        "outputId": "07faa2a4-e704-42cb-9fdd-f7b6791f72a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "en_stop = set(nltk.corpus.stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhMaibGhceFT"
      },
      "outputs": [],
      "source": [
        "def prepare_text_for_lda(text):\n",
        "    tokens = tokenize(text)\n",
        "    tokens = [token for token in tokens if len(token) > 4]\n",
        "    tokens = [token for token in tokens if token not in en_stop]\n",
        "    tokens = [get_lemma(token) for token in tokens]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ceLEp_lcf3R"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "text_data = []\n",
        "for x in train['title']:\n",
        "  tokens = prepare_text_for_lda(x)\n",
        "  text_data.append(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75_j0ge_dquy"
      },
      "outputs": [],
      "source": [
        "from gensim import corpora\n",
        "dictionary = corpora.Dictionary(text_data)\n",
        "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
        "import pickle\n",
        "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
        "dictionary.save('dictionary.gensim')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdxF2FXhdxbs",
        "outputId": "34b483e5-0d18-4edc-f4f6-e70e231b569c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0, '0.038*\"attack\" + 0.036*\"kill\" + 0.025*\"opposition\" + 0.024*\"clash\"')\n",
            "(1, '0.031*\"police\" + 0.025*\"mideast\" + 0.024*\"near\" + 0.021*\"force\"')\n",
            "(2, '0.042*\"strike\" + 0.032*\"pentagon\" + 0.027*\"afghanistan\" + 0.022*\"soldier\"')\n",
            "(3, '0.035*\"official\" + 0.030*\"south\" + 0.026*\"election\" + 0.025*\"korea\"')\n",
            "(4, '0.031*\"france\" + 0.027*\"russia\" + 0.027*\"death\" + 0.020*\"israel\"')\n",
            "(5, '0.037*\"leader\" + 0.028*\"seek\" + 0.024*\"suharto\" + 0.023*\"britain\"')\n",
            "(6, '0.024*\"mexico\" + 0.022*\"british\" + 0.021*\"return\" + 0.021*\"government\"')\n",
            "(7, '0.040*\"zimbabwe\" + 0.031*\"iraqi\" + 0.028*\"press\" + 0.026*\"leaders\"')\n",
            "(8, '0.048*\"palestinian\" + 0.037*\"talks\" + 0.028*\"report\" + 0.022*\"chief\"')\n",
            "(9, '0.035*\"china\" + 0.025*\"ground\" + 0.021*\"kill\" + 0.021*\"pakistan\"')\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "NUM_TOPICS = 10\n",
        "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
        "ldamodel.save('model5.gensim')\n",
        "topics = ldamodel.print_topics(num_words=4)\n",
        "for topic in topics:\n",
        "    print(topic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Steps\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.parse import CoreNLPParser\n",
        "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
        "import en, re\n",
        "from sent_processor import sent_converter\n",
        "\n",
        "\n",
        "def levenshtein(s1, s2):\n",
        "    if len(s1) < len(s2):\n",
        "        return levenshtein(s2, s1)\n",
        "    if len(s2) == 0:\n",
        "        return len(s1)\n",
        "    previous_row = range(len(s2) + 1)\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
        "            deletions = current_row[j] + 1       # than s2\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "    # A way to preserve ordering\n",
        "    s1_spl = s1.split(\" \")\n",
        "    s2_spl = s2.split(\" \")\n",
        "    order = 0\n",
        "    # Prioritize matching in order\n",
        "    for sp in s2_spl:\n",
        "        while len(s1_spl) and sp not in s1_spl[0]:\n",
        "            s1_spl.pop(0)\n",
        "        if len(s1_spl):\n",
        "            order += 1\n",
        "    return previous_row[-1] - order\n",
        "\n",
        "def all_include(x1, x2):\n",
        "    if len(x1) > len(x2):\n",
        "        return all_include(x2, x1)\n",
        "    for x in x1:\n",
        "        if x not in x2:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "class parser:\n",
        "    def __init__(self, utterance):\n",
        "        self.parser_core = CoreNLPParser(url='http://localhost:9000')\n",
        "        self.dep_parser = CoreNLPDependencyParser(url='http://localhost:9000')\n",
        "        self.ner_parser = CoreNLPParser(url='http://localhost:9000', tagtype='ner')\n",
        "        self.sentence = utterance\n",
        "        # self.shift_sent()\n",
        "        # self.sentence = self.removePunct()\n",
        "        self.parseTree = self.parser_core.parse(self.sentence.split())\n",
        "        self.dep_tr = self.dep_parser.raw_parse(self.sentence)\n",
        "        # POS tags / words associated with each verb\n",
        "        self.verb_clusters = dict()\n",
        "        self.preps = []\n",
        "        self.verbs = []\n",
        "        # POS structure / frame of the input\n",
        "        self.frame = None\n",
        "        self.parent_dict = dict()\n",
        "        self.voc_dict = dict()\n",
        "        self.parse_voc_dict(self.parseTree)\n",
        "        if len(self.verbs) == 0:\n",
        "            return\n",
        "        self.parse_dep()\n",
        "\n",
        "\n",
        "    def removePunct(self):\n",
        "        # remove all punctuation except periods & separate numbers\n",
        "        original_sent = self.sentence.lower()\n",
        "        # original_sent = self.sentence\n",
        "        original_sent = original_sent.replace(\"!\", \".\")\n",
        "        original_sent = original_sent.replace(\"?\", \".\")\n",
        "        original_sent = original_sent.replace(\"-\", \" \")\n",
        "        original_sent = original_sent.replace(\"mr.\", \"mr\")\n",
        "        original_sent = original_sent.replace(\"ms.\", \"ms\")\n",
        "        original_sent = original_sent.replace(\"mrs.\", \"mrs\")\n",
        "        original_sent = original_sent.replace(\"dr.\", \"dr\")\n",
        "        original_sent = original_sent.replace(\"drs.\", \"drs\")\n",
        "        original_sent = original_sent.replace(\"st.\", \"st\")\n",
        "        original_sent = original_sent.replace(\"sgt.\", \"sgt\")\n",
        "        original_sent = original_sent.replace(\"lt.\", \"lt\")\n",
        "        original_sent = original_sent.replace(\"fr.\", \"fr\")\n",
        "        original_sent = original_sent.replace(\"pp.\", \"pp\")\n",
        "        original_sent = original_sent.replace(\"pg.\", \"pg\")\n",
        "        original_sent = original_sent.replace(\"pgs.\", \"pgs\")\n",
        "        original_sent = original_sent.replace(\"pps.\", \"pps\")\n",
        "        original_sent = original_sent.replace(\".com\", \" com\")\n",
        "        original_sent = original_sent.replace(\".net\", \" net\")\n",
        "        original_sent = original_sent.replace(\".edu\", \" edu\")\n",
        "        original_sent = original_sent.replace(\"www.\", \"www \")\n",
        "        original_sent = original_sent.replace(\"...\", \" \")\n",
        "        original_sent = original_sent.replace(\"$\", \"$ \")\n",
        "        original_sent = original_sent.replace(\"%\", \" % \")\n",
        "        original_sent = re.sub('[@#^&*\\(\\)_\\+`\\-=\\\\/,<>:;\\\"\\.]+', \" \", original_sent)\n",
        "        original_sent = original_sent.replace(\"'s \", \" s \")\n",
        "        original_sent = original_sent.replace(\"s' \", \"s \")\n",
        "        original_sent = original_sent.replace(\"'d \", \" would \")\n",
        "        original_sent = original_sent.replace(\"'ll \", \" will \")\n",
        "        original_sent = original_sent.replace(\"'ve \", \" have \")\n",
        "        original_sent = original_sent.replace(\" they're \", \" they are \")\n",
        "        original_sent = original_sent.replace(\" we're \", \" we are \")\n",
        "        original_sent = original_sent.replace(\"'t \", \" t \")\n",
        "        # original_sent = original_sent.replace(\"o'clock\", \"o clock\")\n",
        "        # original_sent = re.sub(\"(?<![a-zA-Z]{2,})\\.\",\"\",original_sent,flags=re.IGNORECASE) #removes periods in initials\n",
        "        while \"  \" in original_sent:\n",
        "            original_sent = original_sent.replace(\"  \", \" \")\n",
        "        return original_sent\n",
        "\n",
        "    def split_sent(self):\n",
        "        sent = self.sentence\n",
        "        sent = re.sub('[@#^&*\\(\\)_\\+`=\\\\/,<>;:\\\"\\.]+', \".\", sent)\n",
        "        sent = sent.strip(\"\\n\")\n",
        "        return sent\n",
        "\n",
        "    # get the top-level parts of speech\n",
        "    def getPOSs(self, tree):\n",
        "        \"\"\"\n",
        "        Obtain the top-level parts of speech tags. Fill voc_dict with the tags and their corresponding words.\n",
        "        e.g. {\"NP-1\": \"Captain Kirk\"}\n",
        "        :param tree: the parse tree of the original sentence\n",
        "        :return: the list of top-level POS tags\n",
        "        \"\"\"\n",
        "        poslist = []\n",
        "        for subtree in tree:\n",
        "            if type(subtree) == nltk.tree.Tree:\n",
        "                if subtree.label() == 'PP' or subtree.label() == 'S' or subtree.label() == 'NP' or subtree.label() == 'ADVP':\n",
        "                    if subtree.label() == 'PP':\n",
        "                        # if the first word is not in prepositions, change to NP\n",
        "                        tag = pos_tag(word_tokenize(subtree.leaves()[0]))\n",
        "                        if tag[0][1] == 'IN' or tag[0][1] == 'TO':\n",
        "                            self.preps.append(subtree.leaves()[0])\n",
        "                        else:\n",
        "                            subtree.set_label('NP')\n",
        "                    elif subtree.label() == 'ADVP':\n",
        "                        # If there is an ADVP, add the ADV as well\n",
        "                        import copy\n",
        "                        sub_1 = copy.deepcopy(subtree)\n",
        "                        sub_1.set_label('ADV')\n",
        "                        self.voc_dict.update({sub_1.label() + \"-\" + str(len(self.voc_dict) + 1): \" \".join(sub_1.leaves())})\n",
        "                        poslist += [sub_1]\n",
        "                    self.voc_dict.update({subtree.label() + \"-\" + str(len(self.voc_dict) + 1): \" \".join(subtree.leaves())})\n",
        "                    poslist += [subtree]  # add this pos subtree to the list and stop\n",
        "                elif 'VB' in subtree.label():\n",
        "                    poslist += ['V']\n",
        "                    self.voc_dict.update(\n",
        "                        {\"V-\" + str(len(self.voc_dict) + 1): \" \".join(subtree.leaves())})\n",
        "                    self.verbs.append(en.verb.present(subtree.leaves()[0]))\n",
        "                elif subtree.label() == 'SBAR':\n",
        "                    if \"that\" in subtree.leaves():\n",
        "                        poslist += [\"that\"]\n",
        "                        poslist += self.getPOSs(subtree)\n",
        "                    elif \"whether\" in subtree.leaves():\n",
        "                        poslist += [\"whether\"]\n",
        "                        poslist += self.getPOSs(subtree)\n",
        "                    elif \"what\" in subtree.leaves():\n",
        "                        poslist += [\"what\"]\n",
        "                        poslist += self.getPOSs(subtree)\n",
        "                    elif \"how\" in subtree.leaves():\n",
        "                        poslist += [\"how\"]\n",
        "                        poslist += self.getPOSs(subtree)\n",
        "                else:\n",
        "                    poslist += self.getPOSs(subtree)\n",
        "        return poslist\n",
        "\n",
        "    def parse_voc_dict(self, parseT):\n",
        "        # Try to find the highest parent for every word\n",
        "        # This is a DFS\n",
        "        # print(\"PARSE-T POS--------------\")\n",
        "        parsed_pos = []\n",
        "        for t in parseT:\n",
        "            for tr in t:\n",
        "                parsed_sentence = self.getPOSs(tr)\n",
        "                for parse in parsed_sentence:\n",
        "                    if type(parse) is str:\n",
        "                        parsed_pos += [parse]\n",
        "                    else:\n",
        "                        parsed_pos += [parse.label()]\n",
        "        self.frame = parsed_pos\n",
        "        # print(self.voc_dict)\n",
        "\n",
        "    # def get_dep(self, dep_tree, dep_dict):\n",
        "        \n",
        "\n",
        "    def trav_dep(self, dep_tree, verbs, dep_dict, curr_v):\n",
        "        # A depth first search in the dependency parse tree\n",
        "        if isinstance(dep_tree, str):\n",
        "            # Which words depend on the current verb\n",
        "            dep_dict[curr_v].append(dep_tree)\n",
        "            return dep_dict\n",
        "        else:\n",
        "            label = dep_tree.label()\n",
        "            # Check if the current label is a verb\n",
        "            try:\n",
        "                verb = en.verb.present(label)\n",
        "            except:\n",
        "                verb = \"Not Verb\"\n",
        "            if verb == 'Not Verb' or verb not in verbs:\n",
        "                leaves = dep_tree.leaves()\n",
        "                words = []\n",
        "                for l in leaves:\n",
        "                    try:\n",
        "                        verb = en.verb.present(l)\n",
        "                        words.append(en.verb.present(l))\n",
        "                    except:\n",
        "                        words.append(\"Not Verb\")\n",
        "\n",
        "            # \"BUFFER\" deals with the situation where a verb may appear after its dependents\n",
        "            if verb in verbs:\n",
        "                # The first actual verb would have the same dependents as the \"BUFFER\"\n",
        "                if curr_v == 'BUFFER':\n",
        "                    # Clear the buffer\n",
        "                    dep_dict[verb].extend(dep_dict['BUFFER'])\n",
        "                    dep_dict['BUFFER'] = []\n",
        "                curr_v = verb\n",
        "            elif curr_v is None:\n",
        "                curr_v = 'BUFFER'\n",
        "                dep_dict.update({curr_v: []})\n",
        "            dep_dict[curr_v].append(label)\n",
        "            for c in dep_tree:\n",
        "                dep_dict = self.trav_dep(c, verbs, dep_dict, curr_v)\n",
        "            # print(\"Dep Dict\", dep_dict)\n",
        "            return dep_dict\n",
        "\n",
        "\n",
        "    def parse_dep(self):\n",
        "        tree = None\n",
        "        # The dictionary of subjects; fill the verb's dependent with the subjects in the sentence\n",
        "        subj_dict = dict()\n",
        "        # Put in all the verbs\n",
        "        for t in self.dep_tr:\n",
        "            tree = t\n",
        "            dep_dict = dict()\n",
        "            # Each verb has a series of dependents\n",
        "            for k in self.verbs:\n",
        "                dep_dict.update({k: []})\n",
        "            self.verb_clusters = self.trav_dep(t.tree(), self.verbs, dep_dict, None)\n",
        "            # This is where you match the clusters\n",
        "\n",
        "            for governor, dep, dependent in tree.triples():\n",
        "                # If two words are conj, they have the same nsubj\n",
        "                if (dep == 'nsubj' or dep == 'nsubjpass') and ('VB' in governor[1]):\n",
        "                    subj_dict.update({en.verb.present(governor[0]): dependent[0]})\n",
        "        for v in self.verbs:\n",
        "            temp = []\n",
        "            cluster = self.verb_clusters[v]\n",
        "            for label in self.voc_dict:\n",
        "                if all_include(self.voc_dict[label].split(\" \"), cluster):\n",
        "                    temp.append(label)\n",
        "            self.verb_clusters[v] = temp\n",
        "\n",
        "        for governor, dep, dependent in tree.triples():\n",
        "            # If two words are conj, they have the same nsubj\n",
        "            if (dep == 'conj') and ('VB' in governor[1]) and ('VB' in dependent[1]):\n",
        "                try:\n",
        "                    if en.verb.present(governor[0]) not in subj_dict:\n",
        "                        v_app = en.verb.present(governor[0])\n",
        "                        v_or = en.verb.present(dependent[0])\n",
        "                    else:\n",
        "                        v_app = en.verb.present(dependent[0])\n",
        "                        v_or = en.verb.present(governor[0])\n",
        "                    for l in self.voc_dict:\n",
        "                        if subj_dict[v_or] in self.voc_dict[l] and v_app in self.verb_clusters:\n",
        "                            self.verb_clusters[v_app].insert(0, l)\n",
        "                            subj_dict.update({v_app: l})\n",
        "                except KeyError:\n",
        "                    return\n",
        "\n",
        "        if len(subj_dict) == 1:\n",
        "            # If there is only one subject, then that's it!\n",
        "            subj = list(subj_dict.values())[0]\n",
        "            for l in self.voc_dict:\n",
        "                if subj in self.voc_dict[l]:\n",
        "                    for v in self.verbs:\n",
        "                        self.verb_clusters[v].insert(0, l)\n",
        "        elif len(subj_dict) > 0:\n",
        "            # If there are multiple subjects, pick the closest previous subject\n",
        "            for i in range(1, len(self.verbs)):\n",
        "                if self.verbs[i] not in subj_dict:\n",
        "                    for l in self.voc_dict:\n",
        "                        if subj_dict[self.verbs[i - 1]] in self.voc_dict[l]:\n",
        "                            self.verb_clusters[self.verbs[i]].insert(0, l)\n",
        "                            subj_dict.update({self.verbs[i]: l})\n",
        "\n",
        "        for v in self.verb_clusters:\n",
        "            self.verb_clusters[v] = list(dict.fromkeys(self.verb_clusters[v]))\n",
        "\n",
        "\n",
        "# def ditch_unecessary(frame, fr_s):\n",
        "#     # Compare against all frames, get rid of POS tags in the original frame that is not present in any of the candidates\n",
        "#     fr = \" \".join([f['description']['primary'] for f in fr_s])\n",
        "#     fr = list(set(fr.split(\" \")))\n",
        "#     for i in range(len(fr)):\n",
        "#         if \"-\" in fr[i]:\n",
        "#             fr[i] = fr[i][:fr[i].index(\"-\")]\n",
        "#         if \".\" in fr[i]:\n",
        "#             fr[i] = fr[i][:fr[i].index(\".\")]\n",
        "#     frame = [f[:f.index(\"-\")] for f in frame] if \"-\" in \"\".join(frame) else frame\n",
        "#     new_frame = []\n",
        "#     for f1 in frame:\n",
        "#         if f1 in fr:\n",
        "#             new_frame.append(f1)\n",
        "#     return new_frame\n",
        "\n",
        "import copy\n",
        "def count_fill(orig, frame):\n",
        "    len_frame = len(frame)\n",
        "    num_fill = 0\n",
        "    for o in orig:\n",
        "        while len(frame) and frame[0] != o:\n",
        "            frame.pop(0)\n",
        "        num_fill += 1\n",
        "    return float(num_fill)/len_frame\n",
        "\n",
        "\n",
        "\n",
        "def parse_all(utterance, total_count=0):\n",
        "    \"\"\"\n",
        "    Parse through a given sentence and match it to a VerbNet frame and fill the frame\n",
        "    :param utterance:\n",
        "    :param total_count: controls if we will edit the sentence and try to parse it again: if total_count is 0, we would edit if necessary; if total_count is 1, we would not edit\n",
        "    :return: found (if there is a valid frame), the entire filled frame, and prepositions in the sentence\n",
        "    \"\"\"\n",
        "    p = parser(utterance)\n",
        "    frame_dict = dict()\n",
        "    frame_dict_total = dict()\n",
        "    fd = False\n",
        "\n",
        "    if len(p.voc_dict) == 0:\n",
        "        return (False, frame_dict, [])\n",
        "\n",
        "\n",
        "    # If starting with Prep Phrase, the sentence is in inverted order\n",
        "    # Need to pass into sent converter for it to be in the natural order\n",
        "    if (\"PP-\" in list(p.voc_dict.keys())[0]) and total_count == 0:\n",
        "        p1 = sent_converter()\n",
        "        s1 = p1.conv_sent(p.sentence)\n",
        "        return parse_all(s1, total_count=1)\n",
        "\n",
        "\n",
        "    # If the top-level POS's are all sentences --> The whole sentence is made of all sub-sentences\n",
        "    # Then parse each sub-sentence and return the combined result\n",
        "    if len([x for x in p.voc_dict.keys() if \"S-\" in x]) == len(p.voc_dict) and len(p.voc_dict) > 0 and total_count == 0:\n",
        "        for s in p.voc_dict.values():\n",
        "            found, f_dict, preps = parse_all(s, total_count=1)\n",
        "            if preps is not None:\n",
        "                p.preps.extend(preps)\n",
        "            fd = fd | found\n",
        "            if f_dict is not None:\n",
        "                for k, v in f_dict.items():\n",
        "                    frame_dict.update({k: v})\n",
        "        return (fd, frame_dict, p.preps)\n",
        "\n",
        "    if p.frame == 'EMPTY_FRAME':\n",
        "        return (False, None, None)\n",
        "\n",
        "    import pickle, json\n",
        "    # The mapping file has the verb and its matching class id's\n",
        "    f = open('dictionaries/mapping_v.p', 'rb')\n",
        "    mapping = pickle.load(f)\n",
        "\n",
        "    # Get Verbnet\n",
        "    f = open(\"dictionaries/verbnet.json\", 'r')\n",
        "    f_c = json.load(f)\n",
        "    v_n = f_c['VerbNet']\n",
        "\n",
        "    _syntax = []\n",
        "    _semantics = []\n",
        "    prep_total = dict()\n",
        "    # Traverse verbs and find all the frames\n",
        "    for ind, verb in enumerate(p.verbs):\n",
        "        prep_match = dict()\n",
        "        gen_match = dict()\n",
        "        frame_dict = dict()\n",
        "        potentials = []\n",
        "        prep_total.update({verb: []})\n",
        "        try:\n",
        "            class_ids = mapping[verb]\n",
        "        except KeyError:\n",
        "            print(\"Cannot find verb in mapping!\")\n",
        "            return (False, None, None)\n",
        "        for v in v_n:\n",
        "            max_score = 100\n",
        "            if v['class_id'] in class_ids:\n",
        "                frame = p.verb_clusters[verb]\n",
        "                ori_frame = p.verb_clusters[verb]\n",
        "                fr_s = v['frames']\n",
        "                for f in fr_s:\n",
        "                    f.update({\"CLASS\": v['class_id']})\n",
        "                    # Obtain frame description\n",
        "                    f_str = f['description']['primary'].split(\" \")\n",
        "                    f_str = [x.split(\".\")[0].split(\"_\")[0].split(\"-\")[0] for x in f_str]\n",
        "                    f_str = list(filter(None, f_str))\n",
        "                    lev_score = levenshtein(\" \".join(frame), \" \".join(f_str))\n",
        "\n",
        "                    # Only append the frame to the final result if can_append is true\n",
        "                    can_append = True\n",
        "\n",
        "                    # Preposition matches are more definite (i.e. if the preposition matches with the candidate, it is more likely that this candidate is correct)\n",
        "                    if \"PP\" in \" \".join(frame) and \"PP\" in f_str:\n",
        "                        # Extract all prepositions from the candidate frame\n",
        "                        vals = [args for args in f['syntax'] if args['arg_type'] == 'PREP' or args['arg_type'] == 'TO']\n",
        "                        if len(vals) > 0:\n",
        "                            # Maybe instead of directly p.preps, use individual prep sets instead?\n",
        "                            if vals[0]['value'].islower() and len(set(p.preps).intersection(set(vals[0]['value'].split(\" \")))) > 0:\n",
        "                                prep_match.update({lev_score: f})\n",
        "                            elif vals[0]['value'] == \"PREP\":\n",
        "                                can_append = True\n",
        "                            else:\n",
        "                                can_append = False\n",
        "                    if lev_score <= max_score:\n",
        "                        potentials.append(f)\n",
        "                        # print(\"NEW POTENTIAL MATCH ======\")\n",
        "                        # print(f['description']['primary'])\n",
        "                        if lev_score == max_score:\n",
        "                            # print(gen_match, max_score)\n",
        "                            f_or = gen_match[max_score]['description']['primary'].split(\" \")\n",
        "                            # Filler metric\n",
        "                            ##################################\n",
        "                            # How many slots have been filled in order\n",
        "                            sc_1 = count_fill(ori_frame, f_or)\n",
        "                            sc_2 = count_fill(ori_frame, f_str)\n",
        "                            can_append = can_append and (sc_1 < sc_2)\n",
        "                        if can_append:\n",
        "                            # This is a generic match\n",
        "                            gen_match.update({lev_score: f})\n",
        "                            max_score = lev_score\n",
        "        # If there are no generic matches, we need to process the sentence and try to parse again\n",
        "        if len(gen_match) == 0:\n",
        "            if total_count == 0:\n",
        "                p1 = sent_converter()\n",
        "                s1 = p1.conv_sent(p.sentence)\n",
        "                # print(\"Converted Sentence\", s1)\n",
        "                return parse_all(s1, total_count=1)\n",
        "            # Select based on best match score??\n",
        "        else:\n",
        "            # Find the match with the lowest levenshtein score / best match\n",
        "            if len(prep_match) and min(list(gen_match.keys())) == min(list(prep_match.keys())):\n",
        "                print(\"Prep match!!!!\")\n",
        "                match = prep_match[min(list(prep_match.keys()))]\n",
        "            else:\n",
        "                match = gen_match[min(list(gen_match.keys()))]\n",
        "            # Match the slots!!\n",
        "            print(\"FINAL MATCH\")\n",
        "            print(\"Original Sentence:\", p.sentence)\n",
        "            print(match['description']['primary'])\n",
        "            syntax = match['syntax']\n",
        "            # THE S_INF SITUATION\n",
        "            # Sometimes, the verbnet frame has S- (sentence), but the actual syntax does not have sentence\n",
        "            # Main idea is to make all S- NP-\n",
        "            syntaxes = [s['arg_type'] for s in syntax]\n",
        "            if \"S\" in match['description']['primary'] and \"S-\" in \" \".join(ori_frame) and \"S\" not in syntaxes:\n",
        "                # If the lengths don't match\n",
        "                s_s = [(x, p.voc_dict[x]) for x in p.voc_dict if \"S-\" in x]\n",
        "                if len(match['description']['primary'].split()) - len(ori_frame) <= 2:\n",
        "                    for k, s in s_s:\n",
        "                        k1 = k.replace(\"S-\", \"NP-\")\n",
        "                        # update voc dict\n",
        "                        # update verb cluster\n",
        "                        new_voc = dict()\n",
        "                        for v in p.voc_dict:\n",
        "                            if v == k:\n",
        "                                new_voc.update({k1: p.voc_dict[v]})\n",
        "                                # Also replace everything in verb cluster\n",
        "                                for n, l in p.verb_clusters.items():\n",
        "                                    p.verb_clusters[n] = [k1 if x == v else x for x in l]\n",
        "                            else:\n",
        "                                new_voc.update({v: p.voc_dict[v]})\n",
        "                        p.voc_dict = new_voc\n",
        "\n",
        "                else:\n",
        "                    while \"S-\" in \" \".join(list(p.voc_dict.keys())):\n",
        "                        # The S_INF situation\n",
        "                        for k, s in s_s:\n",
        "                            if s.split()[0] == 'to':\n",
        "                                p.preps.append(\"to\")\n",
        "                                s = \" \".join(s.split()[1:])\n",
        "                            p1 = parser(s)\n",
        "                            new_voc = dict()\n",
        "                            # new_vc = dict()\n",
        "                            for v in p1.voc_dict:\n",
        "                                new_voc.update({v + \"_\" + str(len(p.voc_dict)): p1.voc_dict[v]})\n",
        "                                # Also replace everything in verb cluster\n",
        "                                for n, l in p1.verb_clusters.items():\n",
        "                                    p1.verb_clusters[n] = [v + \"_\" + str(len(p.voc_dict)) if x == v else x for x in l]\n",
        "                            p1.voc_dict = new_voc\n",
        "                            if len(p1.voc_dict) == 1:\n",
        "                                # need to split further\n",
        "                                pt = p1.parser_core.parse(list(p1.voc_dict.values())[0].split())\n",
        "                            for k1, v1 in p1.voc_dict.items():\n",
        "                                p.voc_dict.update({k1: v1})\n",
        "                            p.voc_dict.pop(k)\n",
        "                            p.verbs.extend(p1.verbs)\n",
        "                            for k1, v1 in p1.verb_clusters.items():\n",
        "                                p.verb_clusters.update({k1: v1})\n",
        "                            #TODO: substitute the original sentence with all the voc\n",
        "                            for k1, v1 in p.verb_clusters.items():\n",
        "                                if k in v1:\n",
        "                                    # replace everything\n",
        "                                    for n_v in list(p1.voc_dict.keys()):\n",
        "                                        v1.insert(v1.index(k), n_v)\n",
        "                                    v1.remove(k)\n",
        "                                    p.verb_clusters[k1] = v1\n",
        "\n",
        "\n",
        "            pair_list = [(x[:x.index(\"-\")], p.voc_dict[x]) for x in p.verb_clusters[verb]]\n",
        "            fill_prep = False\n",
        "            frame_dict.update({\"FRAME\": match['description']['primary']})\n",
        "\n",
        "            prep_list = []\n",
        "\n",
        "            # If there is Prep, do the conversion\n",
        "            # Substitute PP = Prep + NN\n",
        "            if \"PP-\" in \" \".join(ori_frame):\n",
        "                le = [x for x in p.verb_clusters[verb] if \"PP-\" in x]\n",
        "                for l in le:\n",
        "                    pp = p.voc_dict[l]\n",
        "                    prep = pp.split(\" \")[0]\n",
        "                    pp = \" \".join(pp.split(\" \")[1:])\n",
        "                    for t in p.parser_core.raw_parse(pp):\n",
        "                        spl = str(t).split(\"(\")\n",
        "                        spl = [s for s in spl if len(s) > 2 and \"S \" not in s and \"FRAG\" not in s and \"ROOT\" not in s]\n",
        "                        new_label = spl[0].split(\" \")[0]\n",
        "                        new_label = new_label.strip()\n",
        "                        if (new_label == 'NN') or (new_label == \"S\" and \"S\" not in [x['arg_type'] for x in syntax]):\n",
        "                            new_label = 'NP'\n",
        "                        # p.voc_dict.update({\"Prep-\" + new_label: pp})\n",
        "                        prep_list.append(tuple([\"Prep-\" + new_label, prep, pp]))\n",
        "\n",
        "            # S_ING --> NP\n",
        "            # print(\"Pair List:\", pair_list)\n",
        "            # print(\"Prep List\", prep_list)\n",
        "            # Handle the case where there is PP in the verbnet frame, but no PREP in the syntax\n",
        "            prep_copy = copy.copy(prep_list)\n",
        "            print(\"prep copy\", prep_copy, \"prep\", prep_list)\n",
        "            prep_dic = {'arg_type': 'PREP', 'value': 'PREP', 'selrestrs': {'selrestr_list': None, 'selrestr_logic': None}, 'synrestrs': {'synrestr_list': None, 'synrestr_logic': '&'}}\n",
        "            if \"PP\" in match['description']['primary'] and \"PREP\" not in [s['arg_type'] for s in syntax]:\n",
        "                # Find the index of the prep\n",
        "                ori_copy = copy.copy(pair_list)\n",
        "                frame_copy = match['description']['primary'].split(\" \")\n",
        "                ind = 0\n",
        "                while len(ori_copy) and ori_copy[0][0] != 'PP':\n",
        "                    up_to = ori_copy.pop(0)\n",
        "                    while len(frame_copy) and frame_copy[0] != up_to:\n",
        "                        ind += 1\n",
        "                        frame_copy.pop(0)\n",
        "                match['syntax'].insert(ind - 1, prep_dic)\n",
        "\n",
        "            # Block out things that you cannot fill\n",
        "            # Do set intersection\n",
        "            ori_copy = copy.deepcopy(pair_list)\n",
        "            fill_prep = False\n",
        "            for s in syntax:\n",
        "                arg = s['arg_type']\n",
        "                # Compare with the 0th element\n",
        "                if (len(ori_copy) == 0) or (not ((ori_copy[0][0] == arg) or (arg == \"VERB\" and ori_copy[0][0] == \"V\") or (arg == \"PREP\" and ori_copy[0][0] == \"PP\") or (\"Prep-\" +  arg in [x[0] for x in prep_copy] and fill_prep))):\n",
        "                    s['arg_type'] = \"?\"\n",
        "                elif arg == \"PREP\" and ori_copy[0][0] == \"PP\":\n",
        "                    fill_prep = True\n",
        "                elif \"Prep-\" +  arg in [x[0] for x in prep_copy] and fill_prep:\n",
        "                    fill_prep = False\n",
        "                    prep_copy.pop(0)\n",
        "                # if len(ori_copy) and ori_copy[0][0] != \"PP\":\n",
        "                if len(ori_copy):\n",
        "                    ori_copy.pop(0)\n",
        "                # Keep popping\n",
        "                # Handle the case of VERB\n",
        "                # Handle the case of PREP + NP\n",
        "\n",
        "            print(\"prep copy\", prep_copy, \"prep\", prep_list)\n",
        "            # Fill things that are not ?\n",
        "            for s in syntax:\n",
        "                if s['arg_type'] == 'VERB':\n",
        "                    s['arg_type'] = verb\n",
        "                    frame_dict.update({s['value']: verb})\n",
        "                    # Pop out the non-verbs before \"V\" on pair_list\n",
        "                    while len(pair_list) and pair_list[0][0] != 'V':\n",
        "                        pair_list.pop(0)\n",
        "                elif s['arg_type'] == 'PREP' and \"PP-\" in \" \".join(ori_frame) and len(prep_list):\n",
        "                    while len(pair_list) and pair_list[0][0] != 'PP':\n",
        "                        pair_list.pop(0)\n",
        "                    fill_prep = True\n",
        "                    s['arg_type'] = prep_list[0][1]\n",
        "                    frame_dict.update({s['value']: prep_list[0][1]})\n",
        "                elif s['arg_type'] != \"?\":\n",
        "                    if fill_prep and \"PP-\" in \" \".join(ori_frame):\n",
        "                        s['arg_type'] = prep_list[0][2]\n",
        "                        prep_list.pop(0)\n",
        "                        fill_prep = False\n",
        "                    else:\n",
        "                        rm_ind = -1\n",
        "                        filled = False\n",
        "                        for i in range(len(pair_list)):\n",
        "                            k, v = pair_list[i]\n",
        "                            if k in s['arg_type'] and not filled:\n",
        "                                s['arg_type'] = v\n",
        "                                frame_dict.update({s['value']: v})\n",
        "                                rm_ind = i\n",
        "                                filled = True\n",
        "                        if rm_ind != -1:\n",
        "                            del pair_list[rm_ind]\n",
        "            # Fill the slots\n",
        "            semantics = match['semantics']\n",
        "            print(semantics)\n",
        "            for pred in semantics:\n",
        "                for a in pred['args']:\n",
        "                    sy_match = [sy['arg_type'] for sy in syntax if sy['value'] == a['value']]\n",
        "                    if len(sy_match) > 0:\n",
        "                        a['value'] = sy_match[0]\n",
        "            frame_dict.update({\"SYNTAX\": syntax})\n",
        "            frame_dict.update({\"SEMANTICS\": semantics})\n",
        "            frame_dict.update({\"ENTIRE_FRAME\": match})\n",
        "            _syntax.append(syntax)\n",
        "            _semantics.append(semantics)\n",
        "        frame_dict_total.update({verb: frame_dict})\n",
        "    # NER parse utterance\n",
        "    ner_list = list(p.ner_parser.tag((utterance.split())))\n",
        "    # print(ner_list)\n",
        "    types = \"\"\n",
        "    names = \"\"\n",
        "    for ne in ner_list:\n",
        "        if ne[1] != 'O':\n",
        "            types = types + ne[1] + \",\"\n",
        "            names = names + ne[0] + \" \"\n",
        "        else:\n",
        "            types = types + \"|\"\n",
        "            names = names + \"|\"\n",
        "    ner_list = []\n",
        "    names = [x.strip() for x in names.split(\"|\") if len(x) > 0]\n",
        "    types = [x for x in types.split(\"|\") if len(x) > 0]\n",
        "    # print(\"NAMES AND TYPES\", names, types)\n",
        "    for i in range(len(types)):\n",
        "        types[i] = types[i].split(\",\")[0]\n",
        "        if types[i] == \"PERSON\":\n",
        "            ner_list.append((\"PERSON\", names[i]))\n",
        "        elif types[i] == \"TITLE\":\n",
        "            ner_list.append((\"PERSON\", \"the \" + names[i]))\n",
        "        else:\n",
        "            ner_list.append((types[i], names[i]))\n",
        "    ner_list.sort(key=lambda t: len(t[1]), reverse=True)\n",
        "    frame_dict_total.update({\"NER\": ner_list})\n",
        "    return (True, frame_dict_total, p.preps)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {api_key}\"\n",
        "  }\n",
        "\n",
        "  payload = {\n",
        "    \"model\": \"gpt-4o\",\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": \"You are dissecting provided news article headlines into the following form: (Agent, Recipient,  Action, Object, Emotion_A, Emotion_R) where agent is the actor, recipient is the reactor, action is what is being carried out, object is what it's carried out on, and emotions of A and R are the emotions of the agent and recipient respectively,  and pull emotions from the pool of: Sad, Happy, Excited, Tender, Angry,  Scared. Display your chain of thought in <thinking> tags for the emotions alone. Display ONLY the bracket set and your thinking process.\",\n",
        "          },\n",
        "          {\n",
        "              \"type\":\"text\",\n",
        "              \"text\":\"Insert news article here\"\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    ],\n",
        "    \"max_tokens\": 500\n",
        " }\n",
        "  response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
        "  print(response.json())\n",
        "  with open('part1.json', 'w') as file:\n",
        "      file.write(json.dumps(response.json(), indent=4))\n",
        "  with open('part1.json', 'r') as file:\n",
        "      data = json.load(file)\n",
        "      resp_array.append(data['choices'][0]['message']['content'])\n",
        "  with open(\"story_coherence.txt\",'a') as file:\n",
        "      file.write(data['choices'][0]['message']['content']+\"\\n\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c15fb8e26c44b4cb34e1bf942a0debc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ec91a71eec74c9fbb7cb6214d30efdb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d61fc96cdb24f89a8a0b0fde5d2fe77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8181f280c06475584bec182b241b191",
            "placeholder": "​",
            "style": "IPY_MODEL_d20234d1ab4849f688905d5b4b0340c7",
            "value": " 379k/? [00:00&lt;00:00, 8.07MB/s]"
          }
        },
        "5bba7a1b6c694a26b8659441021c6744": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e7d5ddd4eca45d48b269922fe3fa407": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c7f55e34b4949cfaa726a25f93b5568": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bba7a1b6c694a26b8659441021c6744",
            "max": 47208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbf0e519b5f047199f52c3a439a50057",
            "value": 47208
          }
        },
        "a4968cd6a6084aeabacb381dbc42ddda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c407ad7b04734809a0f9fa08d209a3a2",
              "IPY_MODEL_9c7f55e34b4949cfaa726a25f93b5568",
              "IPY_MODEL_1d61fc96cdb24f89a8a0b0fde5d2fe77"
            ],
            "layout": "IPY_MODEL_6e7d5ddd4eca45d48b269922fe3fa407"
          }
        },
        "bbf0e519b5f047199f52c3a439a50057": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c407ad7b04734809a0f9fa08d209a3a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ec91a71eec74c9fbb7cb6214d30efdb",
            "placeholder": "​",
            "style": "IPY_MODEL_0c15fb8e26c44b4cb34e1bf942a0debc",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: "
          }
        },
        "d20234d1ab4849f688905d5b4b0340c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8181f280c06475584bec182b241b191": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
